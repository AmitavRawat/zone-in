{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15532306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-jxjfLJNEtiuVMehofT38qYHY/user-uDBM4fcTeieNQikLtMqdsQeC/img-RFvbFPyPAaL82Bn561iDuHKr.png?st=2024-04-07T12%3A43%3A49Z&se=2024-04-07T14%3A43%3A49Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-04-06T23%3A52%3A41Z&ske=2024-04-07T23%3A52%3A41Z&sks=b&skv=2021-08-06&sig=UYRuq35yZupRiB3QZR3mtIwob8cfPshUgPtzMIsrGcE%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "from IPython.display import display\n",
    "import clip\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import os\n",
    "from io import BytesIO\n",
    "from IPython.display import Image as DisplayImage\n",
    "\n",
    "\n",
    "# Set the OPENAI_API_KEY environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Load the model\n",
    "def load_model():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    return model, preprocess, device\n",
    "\n",
    "# Generate and display an image based on the input and description\n",
    "def generate_image(input_image_path, description):\n",
    "    model, preprocess, device = load_model()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    original_image = PILImage.open(input_image_path)\n",
    "    image = preprocess(original_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenize and encode text and image\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "    # Assuming you have a method to create a detailed prompt based on the original image features\n",
    "    detailed_prompt = f\"An expanded version of the house, based on the original features: {description}\"\n",
    "\n",
    "    # Call OpenAI API to generate the image\n",
    "    response = client.images.generate(\n",
    "        prompt=detailed_prompt,\n",
    "        n=1,\n",
    "        size=\"512x512\"\n",
    "    )\n",
    "\n",
    "    # Extract the image URL from the response and display it\n",
    "    if response.data:\n",
    "        image_url = response.data[0].url\n",
    "        display(DisplayImage(url=image_url))\n",
    "    else:\n",
    "        print(\"No image data found in response\")\n",
    "\n",
    "# Example usage\n",
    "input_image_path = \"data/testimage.png\"\n",
    "description = \"Imagine the house with a larger second floor and a new garden area.\"\n",
    "generate_image(input_image_path, description)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
