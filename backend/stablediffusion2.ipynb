{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d341637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure that 'expansion_description' is defined\n",
    "expansion_description = \"adding a second floor and a garden\"\n",
    "\n",
    "# Read the image properly\n",
    "image_path = \"data/testimage.png\"\n",
    "original_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "# Load the Stable Diffusion model\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# It's good to handle model loading with try-except to catch potential errors\n",
    "try:\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token='your_hugging_face_token').to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create the text prompt for the desired change\n",
    "prompt = f\"An expanded version of a house, {expansion_description}\"\n",
    "    \n",
    "# Generate the modified image using Stable Diffusion\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        generated_image = pipeline(prompt=prompt, init_image=original_image, strength=0.8).images[0]\n",
    "except Exception as e:\n",
    "    print(f\"Error generating image: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save the generated image\n",
    "output_path = \"modified_house.png\"\n",
    "generated_image.save(output_path)\n",
    "\n",
    "print(f\"Generated image saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd264f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4').to('cuda')\n",
    "\n",
    "# Initialize a prompt\n",
    "prompt = \"a dog wearing hat\"\n",
    "# Pass the prompt in the pipeline\n",
    "pipe(prompt).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load the pre-trained stable diffusion model\n",
    "model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=True)\n",
    "\n",
    "# Load and preprocess the house image\n",
    "def load_preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    # Perform necessary preprocessing like resizing or normalization\n",
    "    return image\n",
    "\n",
    "# Modify the house image based on the query\n",
    "def modify_house_image(image, query):\n",
    "    # Here you would need to process the query and translate it into a prompt or modifications\n",
    "    # that the model can understand. This is a non-trivial task and would likely involve\n",
    "    # natural language processing and a detailed understanding of the model's capabilities.\n",
    "\n",
    "    prompt = f\"A house expanded based on: {query}\"  # This is a simplistic placeholder\n",
    "    modified_image = model(image=image, prompt=prompt).images[0]\n",
    "    return modified_image\n",
    "\n",
    "# Main function to handle the workflow\n",
    "def main(image_path, query):\n",
    "    original_image = load_preprocess_image(image_path)\n",
    "    modified_image = modify_house_image(original_image, query)\n",
    "    modified_image.show()  # Display the modified image\n",
    "\n",
    "# Example usage\n",
    "main(\"house.jpg\", \"adding a second floor and a swimming pool\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0999bca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning.utilities.distributed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j0/dhfgpfdx2dqdk0_lk2mzx5r40000gn/T/ipykernel_72187/1401082199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdalle_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDALLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dalle_pytorch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdalle_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdalle_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDALLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiscreteVAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdalle_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIDiscreteVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVQGanVAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpkg_resources\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdalle_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dalle_pytorch/dalle_pytorch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdalle_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdalle_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIDiscreteVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVQGanVAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdalle_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDivideMax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dalle_pytorch/vae.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtaming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqgan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVQModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGumbelVQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/taming/models/vqgan.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtaming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtaming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusionmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/taming/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLearningRateMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_obj_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning.utilities.distributed'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from dalle_pytorch import DALLE\n",
    "\n",
    "def load_models():\n",
    "    # Load the CLIP model for text-image embeddings\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Load the DALL-E model for image generation\n",
    "    dalle_model = DALLE(vae_path='path_to_vae', dalle_path='path_to_dalle')\n",
    "\n",
    "    return clip_model, clip_processor, dalle_model\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "    return image\n",
    "\n",
    "def generate_image_embedding(image, clip_model, clip_processor):\n",
    "    # Preprocess the image and generate an embedding\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    image_embedding = clip_model.get_image_features(**inputs)\n",
    "    return image_embedding\n",
    "\n",
    "def generate_modified_image(image_embedding, query, dalle_model, clip_model, clip_processor):\n",
    "    # Use CLIP to process the query\n",
    "    text_inputs = clip_processor(text=[query], return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "    text_features = clip_model.get_text_features(**text_inputs)\n",
    "\n",
    "    # Combine image and text features to generate a prompt for DALL-E\n",
    "    combined_features = torch.cat((image_embedding, text_features), dim=1)\n",
    "\n",
    "    # Generate images using DALL-E based on the combined features\n",
    "    generated_images = dalle_model.generate_images(combined_features)\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "def main():\n",
    "    # Load models\n",
    "    clip_model, clip_processor, dalle_model = load_models()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image_path = 'data/testimage.png'\n",
    "    image = preprocess_image(image_path)\n",
    "\n",
    "    # Generate image embedding\n",
    "    image_embedding = generate_image_embedding(image, clip_model, clip_processor)\n",
    "\n",
    "    # User's query for modifying the house\n",
    "    query = \"expand the house with a modern extension\"\n",
    "\n",
    "    # Generate the modified image based on the user's query\n",
    "    modified_image = generate_modified_image(image_embedding, query, dalle_model, clip_model, clip_processor)\n",
    "\n",
    "    # Save or display the modified image\n",
    "    modified_image.save(\"modified_house.jpg\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edff7a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImagesResponse(created=1712495274, data=[Image(b64_json=None, revised_prompt=None, url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-jxjfLJNEtiuVMehofT38qYHY/user-uDBM4fcTeieNQikLtMqdsQeC/img-xJIKtQee2z7S1Vr3Jm514cSZ.png?st=2024-04-07T12%3A07%3A54Z&se=2024-04-07T14%3A07%3A54Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-04-06T23%3A49%3A21Z&ske=2024-04-07T23%3A49%3A21Z&sks=b&skv=2021-08-06&sig=HEhM1EJRkpvoUyeRYdZzcx6xrULSqzSm/zsuVd10baQ%3D')])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-jxjfLJNEtiuVMehofT38qYHY/user-uDBM4fcTeieNQikLtMqdsQeC/img-xJIKtQee2z7S1Vr3Jm514cSZ.png?st=2024-04-07T12%3A07%3A54Z&se=2024-04-07T14%3A07%3A54Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-04-06T23%3A49%3A21Z&ske=2024-04-07T23%3A49%3A21Z&sks=b&skv=2021-08-06&sig=HEhM1EJRkpvoUyeRYdZzcx6xrULSqzSm/zsuVd10baQ%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "from IPython.display import display\n",
    "import clip\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import os\n",
    "from io import BytesIO\n",
    "from IPython.display import Image as DisplayImage\n",
    "\n",
    "\n",
    "# Set the OPENAI_API_KEY environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Load the model\n",
    "def load_model():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    return model, preprocess, device\n",
    "\n",
    "# Generate and display an image based on the input and description\n",
    "def generate_image(input_image_path, description):\n",
    "    model, preprocess, device = load_model()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = preprocess(PILImage.open(input_image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenize and encode text and image (if necessary for further processing)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "    # Call OpenAI API to generate the image\n",
    "    response = client.images.generate(\n",
    "        prompt=description,\n",
    "        n=1,\n",
    "        size=\"512x512\"\n",
    "    )\n",
    "\n",
    "    # Handle the image data from the response\n",
    "#     print(response)\n",
    "    \n",
    "    if response.data:\n",
    "        image_url = response.data[0].url\n",
    "        display(DisplayImage(url=image_url))\n",
    "    else:\n",
    "        print(\"No image data found in response\")\n",
    "        \n",
    "#     image_data = response.images[0]  # Assuming the response contains a list of images\n",
    "\n",
    "#     # Convert the image data to a displayable format and display it\n",
    "#     if image_data:\n",
    "#         generated_image = PILImage.open(BytesIO(image_data))\n",
    "#         display(generated_image)\n",
    "#     else:\n",
    "#         print(\"No image data found in response\")\n",
    "\n",
    "# Example usage\n",
    "input_image_path = \"data/testimage.png\"\n",
    "description = \"Imagine an expanded version of this house with a larger second floor.\"\n",
    "generate_image(input_image_path, description)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
